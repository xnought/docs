{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmXPzBQu6Olp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.PILToTensor(),\n",
        "])\n",
        "test = torchvision.datasets.MNIST(root='./data', train=False, download=True,  transform=transform)"
      ],
      "metadata": {
        "id": "MVvita956TKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619382b9-c53d-4828-872d-0b9b2b5361a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 97064899.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 6688884.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 32080840.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 17541923.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_test = torch.utils.data.DataLoader(test, batch_size=64)"
      ],
      "metadata": {
        "id": "EhjKhYm_6v8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28*28, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 28*28),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, flat):\n",
        "        return self.model(flat)\n",
        "\n",
        "class AutoencoderBig(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28*28, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 28*28),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, flat):\n",
        "        return self.model(flat)\n",
        "\n",
        "# model = Autoencoder()"
      ],
      "metadata": {
        "id": "G5xZF0TZ7oVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def valid_loss(model, ds_test, loss_fn, device=\"cuda\"):\n",
        "    summed_loss = 0\n",
        "    iters = 0\n",
        "    for batch in ds_test:\n",
        "        image_tensor = batch[0]\n",
        "        B, C, H, W = image_tensor.shape\n",
        "        inputs = image_tensor.view(B, -1)\n",
        "        inputs = inputs.to(device)\n",
        "        inputs = inputs.to(torch.float32)\n",
        "        inputs /= 255.0\n",
        "\n",
        "        # 1. forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # 2. update weights\n",
        "        loss = loss_fn(outputs, inputs)\n",
        "        summed_loss += loss.detach().cpu().item()\n",
        "        iters +=1\n",
        "    return summed_loss / iters\n",
        "\n",
        "\n",
        "def _train(model, ds, device=\"cpu\", iterations=None, epochs=1000):\n",
        "    device = \"cuda\"\n",
        "    loss_fn = nn.MSELoss()\n",
        "    optim = torch.optim.AdamW(model.parameters())\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    losses = []\n",
        "    iteration = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i, batch in enumerate(ds):\n",
        "            if iterations is not None and iteration > iterations:\n",
        "                break\n",
        "\n",
        "            # 0. process data\n",
        "            image_tensor = batch[0]\n",
        "            B, C, H, W = image_tensor.shape\n",
        "            inputs = image_tensor.view(B, -1) # flatten, but not batches\n",
        "            inputs = inputs.to(device)\n",
        "            inputs = inputs.to(torch.float32)\n",
        "            inputs /= 255.0\n",
        "\n",
        "            # 1. forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # 2. update weights\n",
        "            loss = loss_fn(outputs, inputs)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            # 3. log out the losses\n",
        "            losses.append((iteration, loss.detach().cpu().item()))\n",
        "            iteration +=1\n",
        "        valid = valid_loss(model, ds_test, loss_fn, device)\n",
        "        print(epoch, \"train\", losses[-1][1], \"validation\", valid)\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "Rffm3ZdU9eWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_image(image):\n",
        "    image *= 255\n",
        "    image = image.to(torch.uint8)\n",
        "    return Image.fromarray(image.detach().cpu().numpy()).resize((256, 256))\n",
        "\n",
        "def check(image_tensor, device=\"cuda\"):\n",
        "    C, H, W = image_tensor.shape\n",
        "    B = 1\n",
        "    inputs = image_tensor.view(B, -1) # flatten, but not batches\n",
        "    inputs = inputs.to(device)\n",
        "    inputs = inputs.to(torch.float32)\n",
        "    inputs /= 255.0\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    return tensor_to_image(outputs.view(H, W))"
      ],
      "metadata": {
        "id": "mgBeb9IQ_7YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "\n",
        "class QuantizedParams(torch.nn.Module):\n",
        "    def __init__(self, indexes, codebook):\n",
        "        super().__init__()\n",
        "        self.indexes = torch.nn.Parameter(indexes, requires_grad=False)\n",
        "        self.codebook = torch.nn.Parameter(codebook, requires_grad=False)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.codebook[self.indexes.to(torch.int32)]\n",
        "\n",
        "\n",
        "class RegularParams(torch.nn.Module):\n",
        "    def __init__(self, weights):\n",
        "        super().__init__()\n",
        "        self.weights = torch.nn.Parameter(weights, requires_grad=False)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.weights\n",
        "\n",
        "\n",
        "class HybridLinear(torch.nn.Module):\n",
        "    def __init__(self, q_w, q_b):\n",
        "        \"\"\"each are callable modules that return the weights and biases\"\"\"\n",
        "        super().__init__()\n",
        "        self.weight = q_w\n",
        "        self.bias = q_b\n",
        "\n",
        "    def forward(self, X):\n",
        "        return X @ self.weight().T + self.bias()\n",
        "\n",
        "\n",
        "def k_means(X, k=2):\n",
        "    kmeans = MiniBatchKMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n",
        "    return kmeans.cluster_centers_, kmeans.labels_\n",
        "\n",
        "\n",
        "def quantize(weights, k=2, dtype=torch.float32):\n",
        "    centroids, labels = k_means(weights.reshape(-1, 1), k)\n",
        "    codebook = torch.tensor(centroids, dtype=dtype).reshape(-1)\n",
        "    new_weights = torch.tensor(labels, dtype=torch.uint8).reshape(weights.shape)\n",
        "    return codebook, new_weights\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def quantize_linear_layer(layer, bits=8, dtype=torch.float32):\n",
        "    weight = layer.weight\n",
        "    bias = layer.bias\n",
        "\n",
        "    num_weights = weight.view(-1, 1).shape[0]\n",
        "    num_biases = 1 if bias is None else bias.view(-1, 1).shape[0]\n",
        "\n",
        "    # apply k-means if there are enough parameters\n",
        "    total_bits = int(2**bits)\n",
        "    new_weight = None\n",
        "    new_bias = None\n",
        "    if num_weights > total_bits:\n",
        "        w_codebook, w_indexes = quantize(weight, total_bits, dtype)\n",
        "        new_weight = QuantizedParams(w_indexes, w_codebook)\n",
        "    else:\n",
        "        # no quantization :(\n",
        "        new_weight = RegularParams(weight)\n",
        "\n",
        "    if bias is not None and num_biases > total_bits:\n",
        "        b_codebook, b_indexes = quantize(bias, total_bits, dtype)\n",
        "        new_bias = QuantizedParams(b_indexes, b_codebook)\n",
        "    else:\n",
        "        # no quantization :(\n",
        "        new_bias = RegularParams(\n",
        "            bias if bias is not None else torch.tensor(0.0, dtype=dtype)\n",
        "        )\n",
        "\n",
        "    return HybridLinear(new_weight, new_bias)\n",
        "\n",
        "def traverse_named_modules(m, filter=\"Linear\"):\n",
        "    for name, l in m.named_modules():\n",
        "        if type(l).__name__ == filter:\n",
        "            sep_name = name.split(\".\")\n",
        "            parent = \".\".join(sep_name[:-1])\n",
        "            child = sep_name[-1]\n",
        "            yield parent, child, l\n",
        "\n",
        "\n",
        "def traverse_named_modules(m, filter=\"Linear\"):\n",
        "    for name, l in m.named_modules():\n",
        "        if type(l).__name__ == filter:\n",
        "            sep_name = name.split(\".\")\n",
        "            parent = \".\".join(sep_name[:-1])\n",
        "            child = sep_name[-1]\n",
        "            yield parent, child, l\n",
        "\n",
        "\n",
        "def replace(model, filter, callback):\n",
        "    modules = list(traverse_named_modules(model, filter))\n",
        "    for p, c, l in tqdm.tqdm(modules, total=len(modules)):\n",
        "        setattr(model.get_submodule(p), c, callback(p, c, l))\n",
        "\n",
        "\n",
        "def replace_linear_with_quantized(model, bits=8, dtype=torch.float16):\n",
        "    replace(\n",
        "        model,\n",
        "        \"Linear\",\n",
        "        lambda p, c, l: quantize_linear_layer(l, bits=bits, dtype=dtype),\n",
        "    )"
      ],
      "metadata": {
        "id": "N8RmOifWU7Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big = True\n",
        "if big:\n",
        "    model = AutoencoderBig()\n",
        "    model.load_state_dict(torch.load(\"ae-big.pth\"))\n",
        "else:\n",
        "    model = Autoencoder()\n",
        "    model.load_state_dict(torch.load(\"ae-1.pth\"))"
      ],
      "metadata": {
        "id": "hhj_jrJsYS9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Autoencoder()"
      ],
      "metadata": {
        "id": "VdCzpmbbwySK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replace_linear_with_quantized(model, bits=8, dtype=torch.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of39wv4uXuu0",
        "outputId": "fbd9a13c-3809-499e-8778-68541df860e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:08<00:00,  1.63s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "id": "4-EqB551w2AS",
        "outputId": "ac18d3ac-5161-4a13-a008-a4b4a16d5f63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('model.0.weight.indexes',\n",
              "              tensor([[ 99, 176,  64,  ...,   9,   8,  62],\n",
              "                      [ 75,  77, 226,  ..., 213, 190,  96],\n",
              "                      [167, 109, 135,  ...,  47, 134, 223],\n",
              "                      ...,\n",
              "                      [128, 105, 189,  ...,  40,   6,  77],\n",
              "                      [197,   9,  64,  ..., 121, 229, 129],\n",
              "                      [239, 243, 101,  ..., 182, 239, 214]], dtype=torch.uint8)),\n",
              "             ('model.0.weight.codebook',\n",
              "              tensor([-7.2621e-03,  2.3163e-02, -2.4537e-02,  6.5427e-03,  3.2697e-02,\n",
              "                      -3.2733e-02, -1.4953e-02,  1.5024e-02, -1.2421e-03, -1.5231e-02,\n",
              "                       2.7866e-02,  2.9958e-03, -2.9604e-02,  1.0996e-02, -1.2505e-02,\n",
              "                       1.8854e-02, -4.9734e-03, -1.8026e-02, -3.5566e-02, -1.0201e-02,\n",
              "                       3.0018e-02,  2.5374e-02, -2.6596e-02,  7.9431e-03, -2.2758e-02,\n",
              "                       5.7170e-04,  3.4771e-02,  1.2815e-02,  2.0860e-02,  4.5619e-03,\n",
              "                      -3.2390e-03,  1.6492e-02, -1.9664e-02,  9.5780e-03, -3.1334e-02,\n",
              "                      -3.3839e-02, -1.7118e-02, -2.8314e-02,  3.1755e-02, -9.0736e-03,\n",
              "                      -2.8901e-02, -1.3688e-02,  1.7408e-03,  1.4328e-02,  2.6434e-02,\n",
              "                      -2.5563e-02, -6.4154e-03,  5.4512e-03,  2.1842e-02,  2.4123e-02,\n",
              "                      -2.7499e-02,  1.8227e-02,  3.0513e-02,  1.9842e-02, -1.6289e-02,\n",
              "                       1.4624e-03,  3.3938e-02,  2.9022e-02, -3.0417e-02, -1.9715e-03,\n",
              "                      -4.9791e-04, -3.4763e-02, -2.3539e-02,  1.7292e-02, -2.2008e-02,\n",
              "                      -7.5316e-03, -1.4349e-02, -3.8503e-03, -1.8536e-02,  1.5720e-02,\n",
              "                      -8.0144e-03, -5.6928e-03, -2.0265e-02, -3.2218e-02,  7.1419e-03,\n",
              "                       3.7468e-03,  3.5316e-02, -2.9108e-02, -1.0908e-02, -1.5738e-02,\n",
              "                       2.7275e-02,  1.0120e-02,  1.3712e-02, -1.3070e-02,  3.3175e-02,\n",
              "                      -2.9119e-03,  3.1023e-02,  3.2220e-02, -1.7417e-02,  1.1656e-03,\n",
              "                       2.4838e-03,  2.8453e-02,  2.2974e-05, -1.8967e-02,  2.5932e-02,\n",
              "                      -2.6256e-02,  2.3787e-02, -2.0586e-02,  3.3415e-02,  2.9289e-02,\n",
              "                       2.2882e-02,  1.1542e-02,  9.0845e-03, -2.1762e-02,  2.2337e-02,\n",
              "                       6.0072e-03,  2.1384e-02, -1.1261e-02,  1.7608e-02, -3.4549e-02,\n",
              "                       1.9502e-02,  2.5068e-02,  4.8628e-03, -9.5991e-03, -2.5714e-03,\n",
              "                      -2.3863e-02,  1.3414e-02, -1.2212e-02, -6.9718e-03, -2.5224e-02,\n",
              "                       2.0197e-02,  2.6703e-02, -5.4595e-03, -4.4457e-03, -8.5272e-03,\n",
              "                      -2.3248e-02, -1.0550e-02, -2.7782e-02,  1.0414e-02,  2.2393e-03,\n",
              "                       3.2642e-03,  3.4486e-02,  1.8537e-02,  8.2279e-03, -3.1013e-02,\n",
              "                       2.3467e-02,  6.8419e-03, -2.5920e-02, -2.2548e-03,  2.4448e-02,\n",
              "                      -2.8719e-02,  1.4667e-02, -3.0139e-02,  4.2614e-03, -2.6916e-02,\n",
              "                       1.6263e-02, -3.5503e-03,  1.6999e-02,  7.6811e-03, -3.2479e-02,\n",
              "                       3.3673e-02, -2.4197e-02, -1.7733e-02, -3.1657e-02, -3.2997e-02,\n",
              "                      -1.5485e-02,  3.0761e-02,  1.5376e-02, -1.9951e-02, -2.0908e-02,\n",
              "                       2.8160e-02, -1.4017e-02, -9.3306e-03,  1.2499e-02, -1.6559e-02,\n",
              "                       1.9178e-02,  2.8592e-04,  3.5583e-02, -1.0016e-03, -6.6811e-03,\n",
              "                      -8.8010e-03, -1.4660e-02, -2.1507e-02,  3.1514e-02, -3.5002e-02,\n",
              "                       5.7361e-03, -1.1915e-02, -2.2993e-02, -1.9174e-02,  2.0538e-02,\n",
              "                       2.6184e-02, -1.3372e-02,  2.9770e-02,  1.7919e-02,  3.2453e-02,\n",
              "                      -1.2786e-02, -2.9344e-02,  3.5091e-03,  2.6982e-02, -1.1592e-02,\n",
              "                      -2.8527e-02, -7.5233e-04,  1.2171e-02,  6.2676e-03,  8.8146e-03,\n",
              "                      -1.6831e-02,  2.1133e-02, -7.7734e-03,  1.0716e-02, -3.0701e-02,\n",
              "                       1.4008e-02, -3.3557e-02,  3.0262e-02, -2.4879e-02, -6.1665e-03,\n",
              "                      -2.2251e-02,  3.1986e-02, -4.1493e-03,  2.2604e-02, -2.9875e-02,\n",
              "                      -5.2159e-03, -3.4338e-02, -1.8758e-02,  2.5661e-02,  2.7569e-02,\n",
              "                      -3.3276e-02,  1.9926e-03, -2.1222e-02, -1.4710e-03,  8.5174e-03,\n",
              "                       3.5052e-02, -1.8290e-02,  2.7329e-03,  5.1586e-03,  2.4762e-02,\n",
              "                       1.6016e-02, -4.7179e-03,  3.4213e-02, -3.1947e-02,  9.8349e-03,\n",
              "                      -5.9330e-03, -9.8865e-03,  2.8741e-02, -2.8062e-02,  7.4216e-03,\n",
              "                       3.2938e-02,  8.6647e-04,  1.1267e-02, -1.9404e-02,  1.1848e-02,\n",
              "                      -3.5272e-02,  2.1613e-02, -2.7214e-02, -2.2507e-02, -8.2612e-03,\n",
              "                      -1.6013e-02,  9.3353e-03,  3.1278e-02,  1.6736e-02, -1.7138e-03,\n",
              "                      -2.3870e-04,  2.2086e-02,  3.9953e-03, -3.4103e-02,  2.9533e-02,\n",
              "                       1.3121e-02])),\n",
              "             ('model.0.bias.indexes',\n",
              "              tensor([250, 225,  27, 102, 125, 136,  22, 155, 233, 253, 145,  19, 220, 214,\n",
              "                      185, 209, 159,  36, 176, 239, 234,  70, 148, 110, 179,  44, 239, 106,\n",
              "                      177,  80,   7, 176,  80, 189,  34,  28, 192, 202, 161, 105,  92, 135,\n",
              "                      184, 203, 160,  75,  76,  72,  91, 155,  66, 224, 207, 189,  60, 160,\n",
              "                       95, 131, 144, 167,  23, 236, 209, 171, 150,  86,  50,   8,  96, 190,\n",
              "                       32,  34,  96,  50, 187, 250, 186,  56,  86,   6, 121, 234, 150, 190,\n",
              "                      174, 213, 145, 118,  48,   9,  46, 143, 143,   8,  50,  86, 137,  32,\n",
              "                       89, 145,  83, 187,   4,  97,  84,  37, 244, 122, 210,  58, 106,   6,\n",
              "                        4, 184,  51, 172, 172, 121,  78, 129,  25, 162, 176,  79, 216, 138,\n",
              "                       36,  74, 175, 181,  63,  27, 158,  90,   0, 189, 113,  61, 232, 112,\n",
              "                      136, 109,  35, 150, 150,  80,  51, 111,  37, 146, 107, 182,  14, 237,\n",
              "                      132,  83, 175, 121,  59, 235, 123,  20, 194, 236, 196,  48,  40,  21,\n",
              "                       55, 226, 246,  11,  44,   1, 124, 146,  81, 167,  37, 171,  32, 139,\n",
              "                      252, 147, 114,  22,  28, 113,  54, 154,  12, 105,  78, 139, 222, 202,\n",
              "                       45,  12, 212, 191, 107,  21, 204, 209,  49, 137, 126, 236,  30,  28,\n",
              "                      156, 117,  95, 144, 133,  64, 204, 138,  79, 183, 157, 110,  43,  21,\n",
              "                      151,  45, 159, 203,  42, 217, 209, 251, 119, 102,  57, 217, 166,  41,\n",
              "                      135,  34,  27, 118,  48,  18,   5, 210,  75, 100,  99,   6, 174,  51,\n",
              "                      184,  29, 159, 181, 253,  83, 103, 108, 204, 201,  18, 255,  60,  12,\n",
              "                       47,  91, 122, 205, 253, 237, 168, 121,   5,  11,  85, 180,   9,   2,\n",
              "                      140, 167,  93,  61, 135, 103, 200, 109,  43, 210,   9, 167,  46, 143,\n",
              "                      210, 195,  40,  84,  59,  52,  85,  74, 101,  25, 115,  55, 191, 126,\n",
              "                      133,   7, 228, 232,  90, 173,  80, 113, 132, 234,  65, 134, 159,  86,\n",
              "                      201, 137,   5, 113, 126, 240, 125, 132,  98,  77, 246,  24, 189, 135,\n",
              "                       90,  87,  97, 241, 225, 180, 154,  11, 247, 222, 164, 110, 112,   4,\n",
              "                      148, 154, 153,   9, 215,  48, 107,  46,  49, 107,  69, 229,  97,  15,\n",
              "                       45,  87,  76,  45, 144,  27,  72,  43, 116, 133, 250, 242,   5,  22,\n",
              "                       40,  62, 233,  59,  92, 236,  94, 128, 108,  90, 174,  93, 128,   7,\n",
              "                      190,  63, 137,  30,  76, 214,  29, 168, 243, 242,  44, 183, 182, 120,\n",
              "                       90,   6, 241, 203, 225,  20, 215, 231,  87,  16, 130,   6, 152, 195,\n",
              "                       62,  26,  75, 234, 192, 245,  80,  41,  39, 101, 104, 147,  78, 142,\n",
              "                      211, 171,  55, 145, 228,   7, 180,   3, 238, 158, 226, 170, 181,  11,\n",
              "                      109, 238, 179,  10, 252,  52,  38, 112,  32,  78, 105,  46,  26,  67,\n",
              "                      127, 157, 221,  67, 226,  70,  53, 233, 246,  13, 152, 215, 108, 134,\n",
              "                      152, 167, 237, 231, 119, 115, 121, 126,  42,  50,  10, 103, 154,  65,\n",
              "                       25, 141, 101, 207,  32, 138, 175, 109, 137,  99, 110,  88,  99, 219,\n",
              "                      172, 229,  33, 209, 191, 182,  97, 224, 230, 225, 106, 244,  75, 178,\n",
              "                      149, 119, 241,  18, 246,   8, 175,   3, 155, 222, 197, 123, 157, 191,\n",
              "                      156,  35, 171, 236,  29, 227, 225, 189,  82,  16,  61,  67,  58,  44,\n",
              "                       78,  86,  30, 245, 179,  14,  46, 136,  67, 125,  61, 190, 218, 239,\n",
              "                       87,  28, 115,  27, 245,   9, 198, 164,  97, 111,  36,  32,  70,   1,\n",
              "                       56, 134,  34,  31, 186,  87, 213,  86, 207, 175,  69, 218,  62, 122,\n",
              "                      106, 184,  20, 155, 116,  10,  17, 106, 165,  93, 181, 201, 179, 185,\n",
              "                      157, 145, 253,  97,  48,  55,   1,  79, 201, 188, 143, 214, 137,   3,\n",
              "                       80, 123, 110,  86, 201, 187,  97,  70,  44,  10, 187,  41,  27, 198,\n",
              "                       14, 220, 191,  53, 158, 130, 138,   3, 213, 109,  72, 167, 157, 130,\n",
              "                       75,  70, 219, 180,  63, 179, 168,   0,  85, 183, 161, 192,  72, 212,\n",
              "                      215, 169, 148,  86,  47, 157, 120, 187,  36,  23,  44, 103,  80, 164,\n",
              "                      107, 155,  94,  33, 132,   4, 100,   4, 151, 149,  18, 172,  84,  27,\n",
              "                      181,  36, 251, 197, 200, 165,  25,  41, 198,  91, 104,  28, 182, 241,\n",
              "                      203,  66, 147, 163,  23, 197, 114,  68,  51,  71, 119,  55, 249,  52,\n",
              "                        8, 230,  61, 251,  77,   9,  98,  63, 127,  52,  35,  82,  30,  27,\n",
              "                      249,  97, 208,  97,  38,  57, 230,  74,  41, 132, 171,  57,   5,  76,\n",
              "                      129, 249,  96,  51, 152, 200,  21,  21, 187,  24, 237,  83,  99, 239,\n",
              "                       99, 190, 157, 208, 229, 115, 223, 242,  47, 200, 191, 100, 169, 197,\n",
              "                       18, 158,  81, 195, 113,  11,  17,  10, 163, 160,  98,  66,  26,  87,\n",
              "                      141,  91,  90, 115, 109,   1,  73,  31, 169, 130, 166, 155, 165, 200,\n",
              "                      184,   7, 194, 178,  11, 151, 108, 207,  31, 248, 199, 148, 160, 198,\n",
              "                      184, 115, 123, 104, 144,   9, 117, 194,  74, 232, 239, 249, 163,  27,\n",
              "                      171, 209,  92, 213, 251, 156, 196, 165,  37, 221, 198,  29, 130, 253,\n",
              "                       56, 153, 118,  67, 133, 162,  45,  81, 199, 114, 103,   9, 190,  49,\n",
              "                       82, 199,  33,   4, 132,  85, 193, 164, 205, 168,  23, 221,  81,  85,\n",
              "                      112,  15, 182, 201, 119, 216, 145,  64,  52, 206, 213, 254,  34, 182,\n",
              "                      248,  48,  29, 250,   1,  26,  24,  23, 209, 186, 229,  14,  39, 230,\n",
              "                      169, 201, 222,  19,  90, 101, 131,  12,  21,  33, 161, 252, 134,  37,\n",
              "                      166,  77,  63, 189,  89,  78, 253, 145, 144,  60,   6, 250, 236, 151,\n",
              "                       44, 139, 133,  88,  14, 142, 124,  85, 145, 213, 254,  73, 215,  44,\n",
              "                       20, 253,  42,  75, 164, 224,  28, 209, 130, 236,  85,   9,  32,  59,\n",
              "                      184, 173,  85, 136, 123, 233, 176, 112, 103, 247,  22, 206,  73,   2,\n",
              "                      179, 137,  74,  54,  41,  21,  68, 126, 235, 239,  66, 141, 191, 108,\n",
              "                      235, 241,  90, 186,  16,  60, 128, 106, 143,  98, 144, 174, 166,   9,\n",
              "                      250, 183, 146, 246,  72, 223], dtype=torch.uint8)),\n",
              "             ('model.0.bias.codebook',\n",
              "              tensor([ 2.1497e-02, -2.3459e-02,  2.7149e-02, -7.3696e-03, -3.1360e-02,\n",
              "                       2.3675e-03,  1.7395e-02, -1.5260e-02,  3.3428e-02, -3.0455e-03,\n",
              "                       2.2834e-02, -1.9243e-02, -1.0763e-02,  1.1415e-02, -2.7168e-02,\n",
              "                       6.6463e-03,  3.0086e-02,  1.4516e-02,  4.6251e-03,  5.0327e-05,\n",
              "                       2.0984e-02, -2.1349e-02, -4.3091e-03, -2.5313e-02, -1.7094e-02,\n",
              "                      -1.2389e-02,  2.5585e-02,  3.5028e-02,  9.1971e-03,  1.8560e-02,\n",
              "                      -8.9831e-03, -2.9514e-02,  1.2840e-02, -6.1641e-03,  1.5892e-02,\n",
              "                       3.1428e-02, -3.2573e-02,  2.8766e-02,  2.4259e-02, -1.3895e-02,\n",
              "                      -1.1717e-03,  1.9753e-02,  7.4478e-03, -3.0690e-02, -3.5083e-02,\n",
              "                      -1.8008e-02, -2.8828e-02,  1.5041e-03, -4.8628e-03,  1.0110e-02,\n",
              "                      -2.3973e-02,  3.3553e-03, -2.2545e-02,  1.3669e-02,  1.1538e-02,\n",
              "                      -9.7951e-03, -3.3679e-02,  3.2407e-02,  1.0236e-02,  2.1760e-02,\n",
              "                      -1.8635e-03, -1.8811e-02,  7.3985e-04, -1.6113e-02, -2.5894e-02,\n",
              "                      -8.2770e-03,  1.2272e-02, -2.7821e-02,  3.4402e-02, -2.1628e-02,\n",
              "                       1.5279e-02,  2.6640e-02, -2.4609e-02, -1.3122e-02,  1.4087e-02,\n",
              "                      -1.4185e-02,  1.6437e-02,  3.0738e-02,  3.5495e-02,  2.3417e-02,\n",
              "                      -3.5245e-03,  3.2931e-02,  2.4991e-02, -3.2056e-02,  2.9596e-02,\n",
              "                      -5.1508e-03, -1.4940e-02,  1.8076e-02, -8.6308e-03,  2.2574e-02,\n",
              "                      -1.8576e-02, -1.0216e-02,  1.9104e-02,  3.8568e-03, -6.5851e-03,\n",
              "                      -7.8210e-03,  1.3323e-02,  1.9165e-03,  5.0940e-03, -2.0761e-02,\n",
              "                       3.3939e-02,  2.7403e-02, -1.6746e-02, -9.4101e-03, -3.0108e-02,\n",
              "                       2.1357e-02, -1.1820e-02, -2.3252e-02, -6.9125e-03,  2.9216e-02,\n",
              "                      -5.6495e-03,  2.0138e-02, -2.5308e-03, -1.9744e-02, -1.7517e-02,\n",
              "                      -3.4671e-02, -3.3151e-02,  2.2258e-02,  3.7055e-04, -3.1086e-02,\n",
              "                      -2.6699e-02, -2.7507e-02, -6.6324e-04, -1.3564e-02,  2.8213e-02,\n",
              "                       3.1073e-02, -3.5532e-02, -7.6305e-03,  3.4475e-02,  3.1932e-02,\n",
              "                       9.7965e-04,  8.6870e-03, -1.4453e-02,  1.9348e-02,  1.6193e-02,\n",
              "                      -2.2242e-02,  2.5978e-02, -3.3467e-02, -3.4300e-02,  6.2393e-03,\n",
              "                      -1.5746e-02, -2.4841e-02,  2.7630e-02, -1.5510e-02,  1.5002e-02,\n",
              "                       1.7085e-02, -8.7387e-03,  6.9995e-03,  2.5260e-02,  1.0643e-02,\n",
              "                       3.3181e-02,  1.2020e-02, -2.8114e-02,  2.3924e-02, -2.4203e-02,\n",
              "                      -3.2380e-02, -4.0089e-03, -1.0563e-02, -2.2891e-02, -3.1066e-04,\n",
              "                       9.8802e-03,  3.0297e-02, -2.1756e-02,  4.1873e-03,  1.4667e-02,\n",
              "                      -1.2656e-02, -3.1729e-02, -2.4094e-02,  2.7741e-03,  1.0361e-02,\n",
              "                       2.4333e-02, -1.1347e-02,  1.1160e-02,  1.7783e-02,  8.0700e-03,\n",
              "                       2.6945e-02,  3.0448e-02,  2.2699e-02,  2.0678e-02,  4.8630e-03,\n",
              "                      -2.7495e-03, -1.6833e-03,  1.1468e-02, -1.7701e-02, -3.0544e-02,\n",
              "                      -2.6198e-02,  1.2618e-03, -1.7262e-02,  2.7081e-02, -9.4024e-04,\n",
              "                      -2.0038e-02,  5.7690e-03, -3.0876e-02,  1.6270e-02,  2.3251e-02,\n",
              "                       8.9704e-03,  2.1984e-02, -2.8649e-02,  5.1606e-04, -5.4042e-03,\n",
              "                      -1.8388e-02, -2.5752e-02,  3.2689e-02,  3.3660e-02, -1.2112e-02,\n",
              "                       3.1644e-02,  3.0947e-02,  2.5756e-02,  1.5490e-02,  2.9918e-02,\n",
              "                       2.6349e-02,  3.0907e-02, -1.5878e-02, -1.1023e-02,  1.1738e-02,\n",
              "                       1.8367e-02,  3.4535e-02,  2.8597e-02,  2.4491e-02, -1.6531e-02,\n",
              "                      -7.1223e-03, -1.6933e-02, -3.0324e-02, -1.8995e-02,  1.6676e-02,\n",
              "                      -2.6891e-02, -2.3084e-03, -1.0010e-02,  3.6776e-03, -6.0168e-03,\n",
              "                       1.5761e-02,  2.0274e-02,  2.4754e-02,  3.2205e-02,  1.0795e-02,\n",
              "                      -2.3812e-02, -2.5085e-02,  9.4894e-03,  1.3881e-02, -4.4838e-03,\n",
              "                       3.1245e-02, -7.5379e-03,  7.5646e-03,  3.4774e-02,  2.3637e-02,\n",
              "                       3.4096e-02,  7.2885e-03,  2.0008e-02, -1.3261e-02, -8.0940e-03,\n",
              "                      -2.1873e-02,  2.1212e-02,  1.9551e-02, -3.3971e-02,  1.7595e-02,\n",
              "                       1.8838e-02])),\n",
              "             ('model.2.weight.indexes',\n",
              "              tensor([[177, 168,  26,  ...,  80, 211,  56],\n",
              "                      [ 51, 112,  53,  ..., 141,  65, 243],\n",
              "                      [200,  40, 152,  ...,  83, 194, 101],\n",
              "                      ...,\n",
              "                      [ 98, 163,  66,  ..., 245, 167, 178],\n",
              "                      [ 11,   0, 205,  ..., 152, 193, 133],\n",
              "                      [  2, 122, 211,  ..., 137, 242, 134]], dtype=torch.uint8)),\n",
              "             ('model.2.weight.codebook',\n",
              "              tensor([ 8.3830e-03, -2.3008e-02,  2.1225e-02, -7.0700e-03,  2.7955e-02,\n",
              "                       6.3517e-04, -1.4234e-02, -2.8923e-02,  1.4985e-02, -1.8149e-02,\n",
              "                       4.7710e-03, -2.7728e-03, -1.0228e-02,  1.1286e-02,  2.4238e-02,\n",
              "                      -2.6399e-02,  1.8522e-02,  3.1483e-02, -2.0405e-02, -3.1513e-02,\n",
              "                      -1.2261e-02,  6.2984e-03,  2.5674e-03, -5.0220e-03, -1.6147e-02,\n",
              "                       1.6738e-02,  2.5800e-02,  1.3491e-02, -2.5205e-02,  2.9513e-02,\n",
              "                      -9.7032e-04, -3.0490e-02, -3.6758e-03,  1.0191e-02, -8.6807e-03,\n",
              "                      -2.7905e-02, -2.1381e-02,  2.3056e-02,  1.9400e-02,  3.5229e-03,\n",
              "                       7.1505e-03, -1.8869e-02, -2.4120e-02,  9.2111e-03, -1.0982e-02,\n",
              "                       2.7422e-02,  1.5622e-02, -5.9248e-03,  1.2230e-02,  1.6003e-03,\n",
              "                      -1.7273e-02,  3.0444e-02,  5.4410e-03,  1.9704e-02, -1.3339e-02,\n",
              "                       2.2448e-02, -1.9094e-03,  1.7546e-02, -1.4837e-02, -2.7196e-02,\n",
              "                      -3.1059e-04,  2.8811e-02, -2.2156e-02, -2.3579e-02,  1.4011e-02,\n",
              "                      -7.8425e-03,  2.6269e-02,  2.4743e-02, -1.5005e-02, -4.4527e-03,\n",
              "                      -1.9148e-02, -2.9744e-02, -9.2168e-03, -1.6798e-02,  2.0483e-02,\n",
              "                      -1.1696e-02,  2.6923e-02,  2.1731e-02, -1.5741e-02,  2.3767e-02,\n",
              "                       3.0629e-03,  1.1529e-02, -3.1046e-03,  1.0657e-02, -6.5385e-03,\n",
              "                       9.7030e-03, -1.9922e-02,  4.2601e-03, -2.8420e-02, -3.1018e-02,\n",
              "                       1.6174e-02, -1.2841e-02,  7.8666e-03,  1.2951e-02, -7.5637e-03,\n",
              "                       9.0746e-04,  1.4323e-02,  1.7024e-02,  6.8700e-03, -1.3789e-02,\n",
              "                      -2.1875e-03,  3.0964e-02,  1.8317e-03,  1.8256e-02,  2.5198e-02,\n",
              "                       5.0054e-03, -1.4010e-03, -2.0873e-02,  2.8244e-02, -1.5352e-02,\n",
              "                       2.3271e-02, -2.4650e-02,  8.6904e-03, -3.4138e-03, -1.1443e-02,\n",
              "                      -9.7432e-03, -2.6098e-02, -1.7935e-02,  2.9941e-02,  1.9090e-02,\n",
              "                      -2.9138e-02,  1.2438e-02,  4.0079e-03, -5.4760e-04,  1.5313e-02,\n",
              "                      -8.4028e-03, -5.6145e-03, -2.7421e-02, -1.0428e-02, -1.1966e-02,\n",
              "                      -6.2385e-03,  3.8256e-04,  1.1997e-02, -2.9334e-02, -1.6514e-03,\n",
              "                      -4.2013e-03,  1.3729e-02,  1.9985e-02, -2.8686e-02, -2.6689e-02,\n",
              "                      -2.3287e-02, -1.7728e-02,  7.6479e-03,  9.9571e-03, -1.8600e-02,\n",
              "                      -1.5945e-02, -2.2724e-02,  2.9297e-02,  2.1976e-02, -3.0759e-02,\n",
              "                      -4.7227e-03,  2.0718e-02, -1.9415e-02, -1.0612e-02,  5.8268e-03,\n",
              "                       3.0703e-02, -1.4452e-02, -2.1630e-02,  6.5754e-03, -1.0006e-02,\n",
              "                       1.6452e-02,  2.1482e-02, -2.4930e-02,  1.4654e-02,  8.9670e-03,\n",
              "                       8.1031e-03,  2.2671e-02, -1.2552e-02, -2.2435e-02,  1.1771e-02,\n",
              "                      -3.0237e-02, -6.6242e-05,  1.7295e-02,  2.5400e-02,  2.4484e-02,\n",
              "                      -2.3867e-02, -2.1125e-02, -8.1193e-03, -7.3178e-03,  1.0440e-02,\n",
              "                      -5.3106e-03, -1.6568e-02, -1.0787e-02,  6.0496e-03,  2.3212e-03,\n",
              "                      -1.3572e-02,  1.3842e-03, -3.1260e-02,  2.3522e-02, -2.8152e-02,\n",
              "                      -1.7040e-02,  4.5273e-03, -2.4734e-03, -2.5494e-02,  5.2261e-03,\n",
              "                       2.6485e-02,  2.8178e-03, -8.9584e-03, -1.1196e-02,  2.2213e-02,\n",
              "                       3.3054e-03,  7.4196e-03,  2.7683e-02, -2.4373e-02, -2.0159e-02,\n",
              "                      -1.5169e-02, -1.4006e-02, -1.4656e-02,  1.0859e-02,  2.6039e-02,\n",
              "                       3.1216e-02,  1.1055e-02,  2.0972e-02, -1.8370e-02, -2.0634e-02,\n",
              "                       2.2865e-02,  1.7780e-02,  2.9056e-02,  2.8536e-02, -1.3096e-02,\n",
              "                      -9.4736e-03, -2.9524e-02,  2.0720e-03,  1.2793e-02, -1.7510e-02,\n",
              "                       2.4981e-02, -1.1716e-03,  1.2622e-02,  1.3114e-02,  1.8791e-02,\n",
              "                       2.9726e-02,  2.4004e-02, -2.1893e-02, -2.5789e-02, -7.7024e-04,\n",
              "                      -1.5541e-02, -1.6350e-02, -2.6956e-02, -6.8145e-03,  2.6697e-02,\n",
              "                       2.0238e-02,  1.3287e-02, -2.7655e-02,  3.0195e-02,  2.7167e-02,\n",
              "                       1.8016e-02,  1.1575e-03,  9.4447e-03,  5.6290e-03, -3.9420e-03,\n",
              "                       1.5355e-04,  1.5903e-02,  2.5595e-02,  3.7562e-03, -1.9673e-02,\n",
              "                      -2.9991e-02])),\n",
              "             ('model.2.bias.weights',\n",
              "              tensor([-2.1022e-04,  1.5625e-02, -1.8926e-02,  8.1238e-03, -2.3193e-02,\n",
              "                      -1.1840e-02, -7.6746e-03,  2.8441e-02, -5.3594e-03, -8.2097e-03,\n",
              "                      -1.0427e-02,  1.3335e-02,  4.6470e-03,  1.0379e-02,  9.4680e-03,\n",
              "                       6.6233e-03, -1.4834e-02,  2.9758e-02,  4.3145e-03,  3.7476e-03,\n",
              "                      -2.1855e-02,  6.7839e-03, -3.0380e-02, -9.4374e-03, -2.4504e-02,\n",
              "                       1.3522e-02,  1.9023e-02, -1.3957e-02, -9.7078e-03,  2.4420e-02,\n",
              "                       2.0941e-02,  5.9638e-03,  3.0366e-02, -1.1886e-02, -3.0752e-02,\n",
              "                       1.8089e-02,  1.9287e-02,  1.4687e-02, -3.0296e-02,  1.4574e-02,\n",
              "                      -2.5245e-02, -1.5011e-02,  1.2167e-02,  4.1404e-03, -2.9657e-02,\n",
              "                       2.6008e-02, -3.1367e-02, -1.9992e-02, -2.7994e-02,  1.8796e-02,\n",
              "                      -1.1888e-02,  2.6703e-02, -2.7469e-02,  9.5211e-03,  1.4884e-02,\n",
              "                       3.0158e-06,  8.5358e-04, -1.6270e-02,  2.7734e-02, -1.9391e-03,\n",
              "                      -6.6181e-05, -1.2111e-02, -2.5810e-02, -2.6699e-02, -9.9327e-03,\n",
              "                      -2.1462e-02,  2.4689e-02,  8.4499e-03, -2.4634e-02, -2.5919e-02,\n",
              "                      -1.9949e-02, -2.3780e-02,  1.3471e-03,  1.4160e-02, -6.1719e-03,\n",
              "                       5.0541e-03, -1.7589e-02,  1.0510e-02,  4.0718e-03, -1.5898e-02,\n",
              "                       2.9663e-02,  8.5195e-03,  1.9193e-02,  8.1999e-03,  6.5641e-03,\n",
              "                      -2.2261e-02, -1.0179e-02,  2.5672e-02, -9.0664e-03, -7.1715e-03,\n",
              "                      -4.2291e-03,  2.0918e-03,  9.3229e-03, -2.6539e-02, -2.3764e-02,\n",
              "                      -3.1019e-02,  2.5648e-02, -1.5800e-02, -1.2782e-03,  9.2876e-03,\n",
              "                       1.6015e-02, -2.0594e-02, -2.3150e-02,  6.0169e-03, -2.3961e-02,\n",
              "                      -1.4684e-02, -8.5479e-03,  2.3633e-02,  2.5913e-02, -1.2058e-02,\n",
              "                       4.0563e-03, -1.5164e-02, -1.5789e-02,  3.6174e-05, -5.0785e-03,\n",
              "                      -5.3886e-03,  2.4935e-02, -1.4327e-02,  1.5454e-03,  6.3752e-03,\n",
              "                       1.8301e-02, -1.9515e-02, -2.9414e-02, -2.1948e-02, -1.0523e-02,\n",
              "                      -3.1723e-03, -1.6834e-02,  3.1345e-02, -7.5051e-04,  2.9946e-02,\n",
              "                       2.0230e-02,  2.5256e-02,  1.8188e-02,  2.1112e-02, -2.5576e-02,\n",
              "                       2.2535e-02, -1.4188e-04, -2.6382e-02,  5.0716e-03, -9.8771e-03,\n",
              "                       1.0782e-02, -2.2525e-02, -3.0023e-02,  2.0342e-02, -2.9270e-02,\n",
              "                      -2.7866e-02, -2.2136e-02,  1.5396e-02,  2.8763e-02, -8.4503e-03,\n",
              "                      -1.9119e-02,  3.1219e-02, -4.8707e-03,  1.0749e-02, -2.9860e-02,\n",
              "                       2.4577e-02, -1.7085e-03, -1.3415e-03,  2.2874e-02, -6.8486e-03,\n",
              "                       1.2658e-02, -4.3382e-03,  2.4869e-02, -2.5916e-02,  2.9912e-03,\n",
              "                       8.2384e-03,  4.8987e-03,  1.8777e-02, -4.8725e-03, -1.4223e-02,\n",
              "                      -7.5711e-03, -1.4637e-02,  3.1423e-02, -1.7722e-02,  3.8801e-03,\n",
              "                      -1.6458e-02, -2.7265e-02, -1.5311e-02,  8.6656e-03, -2.8580e-02,\n",
              "                       3.9145e-03,  2.2110e-03,  2.4445e-02,  7.6113e-03,  1.0791e-02,\n",
              "                       1.3186e-02, -1.0319e-02, -2.4683e-02,  2.9037e-02, -4.2640e-03,\n",
              "                      -2.0028e-02, -2.6335e-03,  2.5991e-02,  3.3902e-04,  5.9166e-03,\n",
              "                      -2.1256e-02, -1.3751e-02,  7.0480e-03,  3.7478e-03,  1.6863e-02,\n",
              "                       2.0925e-02, -1.3280e-02, -1.0257e-02,  5.9417e-03, -1.6763e-02,\n",
              "                      -1.2835e-02, -2.8871e-02,  2.5247e-02, -1.2637e-03, -1.4496e-03,\n",
              "                       1.7689e-02, -5.6388e-03, -1.7738e-02,  2.3349e-02, -3.1234e-02,\n",
              "                      -2.1665e-02,  3.0029e-02,  1.0628e-02,  5.3818e-03, -1.7743e-02,\n",
              "                      -1.5122e-02, -1.5117e-02, -2.3016e-02, -2.9627e-02, -2.5282e-03,\n",
              "                       1.0469e-05,  2.0543e-02,  3.1403e-02, -7.3713e-03, -1.2596e-02,\n",
              "                      -2.4755e-02, -1.3195e-02, -1.0654e-02, -6.3524e-03, -3.0451e-02,\n",
              "                       1.4560e-02, -1.7574e-02,  7.7352e-03, -6.0062e-04, -2.4667e-02,\n",
              "                       2.5799e-02,  3.0587e-02,  6.1894e-03,  2.0245e-03,  3.9072e-03,\n",
              "                       2.3261e-02, -3.0104e-04,  2.0570e-02,  2.7676e-02,  5.7035e-03,\n",
              "                       1.2516e-02,  1.3480e-02,  1.7982e-02, -1.7737e-02, -7.6268e-03,\n",
              "                      -1.8016e-02])),\n",
              "             ('model.4.weight.indexes',\n",
              "              tensor([[183, 106,  69,  ..., 120,  25,  32],\n",
              "                      [161, 147, 217,  ...,   0, 211, 138],\n",
              "                      [157, 181,  54,  ..., 196, 110,  22],\n",
              "                      ...,\n",
              "                      [192,   0,  90,  ..., 146, 158,   6],\n",
              "                      [ 17,  21, 193,  ..., 123, 157, 176],\n",
              "                      [ 68,  96,  20,  ..., 109,  59,  66]], dtype=torch.uint8)),\n",
              "             ('model.4.weight.codebook',\n",
              "              tensor([ 0.0032, -0.0472,  0.0474, -0.0258,  0.0272, -0.0113, -0.0606,  0.0146,\n",
              "                       0.0611,  0.0381, -0.0348, -0.0203, -0.0514,  0.0538, -0.0069, -0.0410,\n",
              "                       0.0221, -0.0001,  0.0334,  0.0092, -0.0575, -0.0161,  0.0441, -0.0300,\n",
              "                      -0.0554,  0.0187,  0.0581, -0.0047,  0.0290,  0.0503, -0.0455, -0.0372,\n",
              "                       0.0116,  0.0358,  0.0068,  0.0247, -0.0231,  0.0415, -0.0096,  0.0554,\n",
              "                      -0.0141, -0.0033,  0.0311,  0.0164, -0.0430, -0.0285, -0.0489, -0.0183,\n",
              "                       0.0205, -0.0325, -0.0525,  0.0054, -0.0502,  0.0528, -0.0220,  0.0466,\n",
              "                       0.0398, -0.0393, -0.0021, -0.0087, -0.0590,  0.0015,  0.0596,  0.0262,\n",
              "                       0.0348,  0.0126, -0.0557, -0.0127, -0.0445,  0.0570, -0.0538,  0.0517,\n",
              "                       0.0235,  0.0429,  0.0174, -0.0360, -0.0499,  0.0622, -0.0272,  0.0137,\n",
              "                      -0.0425, -0.0566,  0.0319, -0.0015, -0.0316,  0.0492,  0.0459, -0.0601,\n",
              "                       0.0043,  0.0368, -0.0622, -0.0290,  0.0083, -0.0383,  0.0300, -0.0073,\n",
              "                      -0.0168,  0.0281, -0.0173, -0.0237,  0.0447,  0.0019,  0.0105,  0.0197,\n",
              "                      -0.0055,  0.0549,  0.0058,  0.0217, -0.0137, -0.0192, -0.0336, -0.0122,\n",
              "                      -0.0481,  0.0481, -0.0580, -0.0461,  0.0151, -0.0153,  0.0005,  0.0073,\n",
              "                      -0.0404, -0.0209,  0.0591,  0.0326, -0.0415, -0.0310,  0.0344, -0.0102,\n",
              "                      -0.0509, -0.0145,  0.0422,  0.0295, -0.0265,  0.0601, -0.0008,  0.0389,\n",
              "                       0.0182,  0.0617,  0.0230, -0.0366,  0.0257,  0.0559, -0.0149,  0.0507,\n",
              "                      -0.0542, -0.0520,  0.0353, -0.0354,  0.0436,  0.0155, -0.0616, -0.0570,\n",
              "                       0.0407, -0.0091,  0.0142, -0.0331,  0.0523, -0.0596,  0.0252, -0.0251,\n",
              "                      -0.0244,  0.0372,  0.0575,  0.0241, -0.0398, -0.0530, -0.0042, -0.0060,\n",
              "                      -0.0279,  0.0110, -0.0450, -0.0027,  0.0209,  0.0087,  0.0305,  0.0564,\n",
              "                      -0.0082,  0.0132,  0.0543,  0.0267,  0.0010,  0.0049,  0.0453, -0.0214,\n",
              "                      -0.0496, -0.0296,  0.0496,  0.0225,  0.0078, -0.0547, -0.0225, -0.0440,\n",
              "                      -0.0486,  0.0063,  0.0192, -0.0388,  0.0394,  0.0363,  0.0339, -0.0611,\n",
              "                       0.0606,  0.0096, -0.0476, -0.0321, -0.0078,  0.0023, -0.0435,  0.0512,\n",
              "                       0.0377,  0.0178, -0.0377, -0.0108, -0.0199,  0.0487, -0.0467,  0.0038,\n",
              "                       0.0586,  0.0201,  0.0330, -0.0065,  0.0160,  0.0419,  0.0027,  0.0315,\n",
              "                      -0.0132, -0.0585, -0.0493,  0.0385, -0.0550,  0.0402,  0.0433, -0.0342,\n",
              "                      -0.0534,  0.0411, -0.0420, -0.0177,  0.0169, -0.0505,  0.0121, -0.0038,\n",
              "                       0.0425,  0.0286,  0.0499, -0.0118, -0.0187, -0.0195, -0.0051,  0.0100,\n",
              "                      -0.0165, -0.0157,  0.0323,  0.0276,  0.0213,  0.0534, -0.0561, -0.0305])),\n",
              "             ('model.4.bias.weights',\n",
              "              tensor([-0.0036, -0.0506,  0.0252, -0.0514, -0.0576, -0.0233,  0.0164, -0.0212,\n",
              "                      -0.0488,  0.0608, -0.0485,  0.0129,  0.0087,  0.0602,  0.0330, -0.0465,\n",
              "                       0.0051, -0.0110, -0.0132, -0.0345,  0.0052,  0.0613, -0.0139,  0.0335,\n",
              "                      -0.0320,  0.0409, -0.0393,  0.0426,  0.0135,  0.0021,  0.0101,  0.0041,\n",
              "                      -0.0336, -0.0435, -0.0233, -0.0022, -0.0282, -0.0018,  0.0576,  0.0029,\n",
              "                      -0.0250,  0.0459,  0.0200,  0.0276, -0.0429, -0.0328, -0.0478,  0.0047,\n",
              "                       0.0327,  0.0187, -0.0622,  0.0417, -0.0545,  0.0329, -0.0540,  0.0391,\n",
              "                      -0.0575,  0.0360,  0.0076,  0.0045,  0.0547,  0.0398, -0.0487, -0.0610,\n",
              "                      -0.0351,  0.0586, -0.0587, -0.0390, -0.0129,  0.0391, -0.0576,  0.0071,\n",
              "                       0.0422,  0.0139,  0.0393,  0.0220,  0.0159, -0.0032,  0.0511,  0.0212,\n",
              "                       0.0089, -0.0388,  0.0208, -0.0222,  0.0280, -0.0480, -0.0470,  0.0225,\n",
              "                       0.0603, -0.0279, -0.0233,  0.0237,  0.0554,  0.0259, -0.0496,  0.0573,\n",
              "                       0.0538,  0.0075,  0.0118, -0.0039, -0.0378,  0.0391,  0.0615, -0.0017,\n",
              "                      -0.0407,  0.0275, -0.0549, -0.0144, -0.0200,  0.0450, -0.0146,  0.0371,\n",
              "                      -0.0326, -0.0542,  0.0333, -0.0272, -0.0113, -0.0442, -0.0320, -0.0571,\n",
              "                       0.0085, -0.0321, -0.0457, -0.0453, -0.0112,  0.0504, -0.0277,  0.0502,\n",
              "                       0.0145,  0.0212,  0.0298, -0.0379,  0.0618, -0.0012,  0.0265,  0.0070,\n",
              "                      -0.0419,  0.0532,  0.0533, -0.0579,  0.0505, -0.0595, -0.0498,  0.0605,\n",
              "                       0.0172, -0.0065, -0.0584,  0.0497,  0.0199, -0.0283,  0.0022,  0.0239,\n",
              "                       0.0365,  0.0462,  0.0242, -0.0301, -0.0511,  0.0609,  0.0139,  0.0003,\n",
              "                       0.0111,  0.0325, -0.0185,  0.0072,  0.0180,  0.0046, -0.0359, -0.0592,\n",
              "                       0.0307,  0.0133,  0.0245, -0.0189, -0.0281, -0.0601, -0.0295, -0.0161,\n",
              "                      -0.0274,  0.0453, -0.0432, -0.0411,  0.0067, -0.0196, -0.0584,  0.0456,\n",
              "                      -0.0493, -0.0547,  0.0344,  0.0408, -0.0184, -0.0451,  0.0567, -0.0324,\n",
              "                       0.0422,  0.0218,  0.0484, -0.0451,  0.0289,  0.0550,  0.0603, -0.0197,\n",
              "                       0.0514,  0.0140,  0.0389,  0.0320, -0.0131,  0.0064,  0.0392, -0.0430,\n",
              "                      -0.0323,  0.0200,  0.0462, -0.0431, -0.0073, -0.0271,  0.0195,  0.0458,\n",
              "                      -0.0552, -0.0212,  0.0207, -0.0354,  0.0189,  0.0280,  0.0329,  0.0270,\n",
              "                      -0.0404,  0.0189,  0.0591,  0.0111, -0.0070,  0.0432, -0.0147, -0.0532,\n",
              "                       0.0289, -0.0463,  0.0562,  0.0231,  0.0046, -0.0030, -0.0075,  0.0472,\n",
              "                      -0.0232,  0.0625, -0.0598,  0.0390,  0.0102,  0.0118, -0.0592,  0.0500,\n",
              "                      -0.0508, -0.0595, -0.0582, -0.0232, -0.0316,  0.0465,  0.0325,  0.0052])),\n",
              "             ('model.6.weight.indexes',\n",
              "              tensor([[ 51, 198,  11,  ...,  45, 208, 138],\n",
              "                      [168,  50, 124,  ..., 214,  11, 240],\n",
              "                      [173, 223, 225,  ...,  88, 210, 228],\n",
              "                      ...,\n",
              "                      [207, 208,  54,  ...,  32, 145,  89],\n",
              "                      [190,  94,  99,  ...,  96,  82,  75],\n",
              "                      [  3, 250,  15,  ..., 255, 148, 140]], dtype=torch.uint8)),\n",
              "             ('model.6.weight.codebook',\n",
              "              tensor([ 2.0634e-02, -2.5737e-02,  4.8115e-02, -5.5918e-02, -2.4491e-03,\n",
              "                      -4.1419e-02,  3.2883e-02, -1.6152e-02,  5.6721e-02,  9.1291e-03,\n",
              "                      -3.4239e-02, -4.8673e-02,  3.9191e-02,  2.7590e-02, -2.8014e-03,\n",
              "                       1.5401e-02,  2.8008e-03, -6.0649e-02,  4.3502e-02,  6.1802e-02,\n",
              "                      -2.9615e-02,  3.6489e-02, -4.5106e-02, -2.2497e-02,  5.2756e-02,\n",
              "                      -1.2924e-02, -5.2518e-02, -3.7140e-02,  1.1835e-02,  2.3469e-02,\n",
              "                       5.5447e-03,  1.7828e-02, -1.8100e-02, -8.8795e-04,  5.9473e-02,\n",
              "                      -3.5978e-03,  3.0374e-02, -9.0297e-03, -3.1881e-02, -5.7670e-02,\n",
              "                       4.5366e-02, -5.9652e-03, -3.9844e-02,  4.1967e-02,  5.1200e-02,\n",
              "                      -4.3009e-02, -1.9908e-02,  2.5870e-02, -1.0882e-02, -2.7416e-02,\n",
              "                      -4.6939e-02,  5.4241e-02,  3.5136e-02, -2.3723e-02, -5.0625e-02,\n",
              "                      -5.3479e-02,  6.5996e-03,  1.3541e-02, -6.1670e-02,  1.6484e-03,\n",
              "                       4.6301e-02, -3.8854e-02, -1.4794e-02, -5.9021e-02,  1.0365e-02,\n",
              "                       4.4801e-03,  1.6470e-02,  5.8125e-02,  2.9209e-02,  3.8060e-02,\n",
              "                       3.1479e-02,  5.0100e-02,  1.9025e-02, -3.5365e-02, -3.6549e-02,\n",
              "                      -7.4076e-03,  6.0827e-02,  2.2057e-02, -2.8443e-02,  2.4544e-02,\n",
              "                      -1.1697e-02, -3.3060e-02, -5.4730e-02, -4.9931e-03, -4.4618e-02,\n",
              "                      -2.0979e-02,  7.6100e-03, -2.6131e-05, -1.7652e-02,  4.0370e-02,\n",
              "                       5.3727e-02,  4.2981e-02, -5.6734e-02,  5.5275e-02, -3.0760e-02,\n",
              "                       3.4298e-02,  4.9031e-02, -4.9602e-02, -9.9786e-03, -4.1953e-02,\n",
              "                      -4.7804e-02, -4.4080e-02, -1.7177e-03,  4.7272e-02,  1.4144e-02,\n",
              "                      -4.6048e-02,  5.1719e-02, -2.4837e-02,  2.6668e-02,  4.4487e-02,\n",
              "                       1.9847e-02,  2.8626e-02,  5.7209e-02,  3.9135e-03, -1.5697e-02,\n",
              "                      -5.8136e-02,  1.0957e-02,  1.2574e-02, -1.3863e-02, -5.1116e-02,\n",
              "                       3.9762e-02,  9.7202e-03,  4.0952e-02,  1.4793e-02,  2.1555e-02,\n",
              "                      -1.8928e-02, -4.5267e-03,  5.0644e-02, -3.7761e-02,  1.6955e-02,\n",
              "                      -2.9016e-02,  2.2989e-02, -5.9775e-02,  3.1963e-02, -6.4437e-03,\n",
              "                      -3.1757e-03, -2.6956e-02, -2.0457e-02,  3.7514e-02, -5.5538e-02,\n",
              "                       2.5497e-02, -6.2225e-02, -1.6638e-02, -4.0357e-02,  5.0277e-04,\n",
              "                       2.2247e-03,  3.3842e-02,  3.5565e-02, -2.1989e-02,  5.5736e-02,\n",
              "                       3.0947e-02,  5.9899e-02,  2.9807e-02, -2.4317e-02, -5.1592e-02,\n",
              "                      -1.2468e-02, -1.7141e-02,  1.5955e-02, -3.2464e-02, -6.1132e-02,\n",
              "                      -5.5137e-02,  7.1119e-03,  1.2203e-02,  8.5664e-03,  5.9032e-02,\n",
              "                       2.8086e-02,  3.8617e-02, -1.9385e-02, -5.4312e-02, -4.2486e-02,\n",
              "                       2.5069e-02,  4.6780e-02,  6.1329e-02, -4.0473e-03, -3.8324e-02,\n",
              "                       5.2251e-02, -5.3005e-02, -3.3656e-02, -2.1064e-03,  4.2478e-02,\n",
              "                      -4.5569e-02,  2.1073e-02, -5.7204e-02,  4.4004e-02, -3.1331e-02,\n",
              "                       6.0796e-03,  4.5832e-02, -3.0184e-02,  4.1460e-02,  5.3242e-02,\n",
              "                       4.9562e-02, -7.9360e-03, -5.0106e-02,  3.3485e-03, -5.6303e-02,\n",
              "                      -2.5318e-02, -2.7912e-02,  2.3995e-02,  1.8209e-02,  1.0560e-03,\n",
              "                      -1.1311e-02,  5.0241e-03, -2.3076e-02, -3.5948e-02,  3.3359e-02,\n",
              "                       6.0352e-02,  5.7670e-02, -8.5110e-03,  1.7412e-02,  1.1428e-02,\n",
              "                      -2.6545e-02, -2.1493e-02, -1.8511e-02, -4.7357e-02, -4.3540e-02,\n",
              "                      -4.8262e-02,  1.9445e-02, -1.3385e-02,  5.4767e-02, -5.8603e-02,\n",
              "                      -5.9400e-02,  5.8590e-02,  5.6221e-02,  1.3000e-02,  3.4716e-02,\n",
              "                      -1.0428e-02,  3.2421e-02, -3.9338e-02, -1.5264e-02,  2.7133e-02,\n",
              "                      -4.0887e-02,  4.8557e-02, -9.5137e-03, -5.4934e-03, -6.9117e-03,\n",
              "                      -6.0193e-02, -4.8167e-04,  3.6032e-02, -1.2073e-02, -1.4326e-02,\n",
              "                      -1.3105e-03,  4.4942e-02,  1.8604e-02,  6.2279e-02,  2.6244e-02,\n",
              "                      -3.4810e-02, -4.6517e-02,  2.2534e-02, -5.2048e-02, -4.9128e-02,\n",
              "                       2.0246e-02,  3.6997e-02,  4.7693e-02, -2.6145e-02, -5.3907e-02,\n",
              "                       8.0720e-03])),\n",
              "             ('model.6.bias.indexes',\n",
              "              tensor([  5, 193, 178, 137, 176, 136,  86, 150, 124,  86, 101,   5,   2,  56,\n",
              "                      186, 116,   0,  64,   1, 106,   0, 174,  59, 134,   2,  60,  78, 128,\n",
              "                       48,  76,  24,  45,  26,  80, 217, 209,  40, 201, 197, 129, 138,  72,\n",
              "                      175,  36,   0, 124,   2, 115, 209,  38, 172,  47,  33,   1, 233,  67,\n",
              "                      194, 219, 190, 192, 231, 184, 148, 169, 200, 161, 148, 104, 172,  86,\n",
              "                       27,  13, 169, 169,  77, 237, 160,  43,  45, 130,  36, 108, 165,  25,\n",
              "                       83,  30,  90, 134,  58, 104,  58, 224,  77,  20, 120, 181, 180,  77,\n",
              "                       21, 153,  78,  70,   4,  88,  68, 109, 173, 184,  42, 163,  43, 190,\n",
              "                       60, 127,  35, 244,  94,  64,  73, 189, 160,  64, 200,  85, 135, 193,\n",
              "                       16,  73,   9,  50, 221, 214, 216,  61,  75,  56, 165,  45,  94,  93,\n",
              "                       28,  89, 240,  72,  90, 109, 105,  40,   3, 187, 118, 216, 185, 216,\n",
              "                      105, 212,  75, 104,  45, 230, 214, 106, 248, 142,  28,  78, 155,  23,\n",
              "                      165,  12, 107, 198, 225, 118, 120, 178,  92, 185, 110, 138, 125, 171,\n",
              "                       31, 205,   4,  54,  83, 117, 231,  80,  37, 254, 171, 193, 117,  74,\n",
              "                        7,  38, 222, 187, 116,  26, 135, 182, 186,  50, 107, 207, 149, 207,\n",
              "                       86,  31, 158,  21, 252,  20,  87, 164, 162,   8,   0,  63,  70, 226,\n",
              "                       74,  76,  16, 119, 255,  91, 111,  10,  57, 133,  69, 125,  26, 112,\n",
              "                      225, 210,  14, 180, 157,  91, 212,  57, 148, 241, 219, 226, 193, 183,\n",
              "                       38,  10, 232,  17,  41, 110, 206,  43, 131, 102,  60,  98, 155, 210,\n",
              "                       62, 220,  76, 175,  75, 114,   9, 184,  85, 140, 160, 123,  81, 171,\n",
              "                      221, 162,  36, 226,  44,   5, 156,  32,  67, 199, 221, 102, 154, 133,\n",
              "                      171, 209,  14, 166, 103, 235,  28, 186, 106,  99, 255, 105, 186,  91,\n",
              "                       30, 236,  56,  38, 133,  23, 214, 236, 222, 215, 234,  97, 215, 225,\n",
              "                      229,  30,  57, 218,  65, 133,   2,  66,  52, 127,   4, 168, 202,  63,\n",
              "                       16,  55, 173,  80, 242, 176, 117, 137, 123, 127, 220,  34, 138, 247,\n",
              "                      253, 153, 106,  87, 149, 120,  57, 146,  71, 226,  56, 165,  92,  63,\n",
              "                       74, 207,  92,  42,  86,  13, 181, 152,  94,  84, 205, 241,   6, 149,\n",
              "                       33, 142, 222,  80, 206, 114,  22,  53,  62, 147, 100,   1, 202, 120,\n",
              "                      112,  68,  54, 224, 171, 214,  87,  92,  45, 132, 159, 250, 157,   1,\n",
              "                      129,  91, 158,  50,  69,  35, 214,  63, 238,  16, 250, 230,  10, 175,\n",
              "                      215, 108, 126,  83, 195, 194, 169, 211,  42,  15, 176,  62, 180, 153,\n",
              "                       18, 243,  55, 178, 207,   7,  98,  82, 205, 132, 125, 137,  85, 143,\n",
              "                      146, 103, 233, 101,  95,  18,  14, 121, 186, 102,  46, 154, 228,  23,\n",
              "                      203,  89,  79,   3, 153,  87,  44,  34,  16,  75,  64,  92, 177, 102,\n",
              "                      211, 245, 167, 132,  21,  33, 135, 104,  39, 151,  22, 146, 248, 154,\n",
              "                      108, 229, 204,  51, 125,   3, 228, 147, 150, 128,  36, 232,  84, 106,\n",
              "                        3,  22,  41, 151,  98, 242, 239, 160, 176,  33,  55,  53,  70, 152,\n",
              "                      107, 228, 240, 171, 140,  73, 251, 186, 248, 143,  26,  91,  55,  10,\n",
              "                      192, 143, 171,  64, 168,  78, 239,  95, 144, 177, 244,  82,  82, 254,\n",
              "                      203, 110,   6, 170, 134,  43, 126,  39, 103,  17,  28,  79,  67,   9,\n",
              "                      134, 134, 116,  25, 183,  95, 104, 230, 199, 148, 164, 136, 144,  97,\n",
              "                       13,   2,  40,  13,  53, 192, 199, 151, 250, 158, 121, 131, 201, 107,\n",
              "                       81, 198, 120, 209, 108,  47, 233,  63, 112, 170, 162, 117,  33,  63,\n",
              "                      102, 194, 146,  63,  64,  59,  99,  82,  62, 107, 200,  95,   6, 215,\n",
              "                       82, 240,  85, 110, 242,  67,  19,  24,  48, 169,   8, 116, 243,  56,\n",
              "                      128, 230,  30, 218,  52,  35, 104,   9, 183, 190, 145,  68, 166, 199,\n",
              "                       56, 101, 106, 250,  98, 108, 217,  98, 232, 119, 189, 130, 222, 155,\n",
              "                      108,  30, 107, 142,  60, 115, 114,  16, 224,  89,  95,  54, 142, 160,\n",
              "                      101,  90,  88, 170, 244, 171, 108, 238, 191,  16,  74,  86,  60, 120,\n",
              "                       59, 116, 205, 115,  18, 238,  97, 158, 141,  46, 191,  51, 146, 240,\n",
              "                      112, 134,   1,  84, 111, 205, 126,  45, 248,  56,  86, 251,  61,   5,\n",
              "                      251, 121,  67,  56,  96,  42, 125, 210,  69, 253, 216, 136,   0,  49,\n",
              "                       69, 102,  21,  50, 252,  75,  81,  93,  39, 254, 167,   0,  44,  17,\n",
              "                       19,  12, 124,  44,   9, 139, 147,  95, 173, 209, 137, 235, 158,  72,\n",
              "                      100,  10,  89, 137, 189, 185,  13,  31, 159, 166, 208,  54,  87, 176,\n",
              "                       61,  57, 107,  42,  85,  38, 121,  77, 162,  66,  16, 131,  73, 135,\n",
              "                      122,  65, 221, 157,  32, 145, 123,  37, 191, 131, 159, 153,  41,  48,\n",
              "                      204, 134, 202,  52,  88,  44, 184,  41, 124, 181, 247, 180, 240,  52,\n",
              "                       31,  80,  10, 143, 171,  91, 138, 215, 223,  31, 149, 188, 172, 135,\n",
              "                      140, 218,  76, 151, 125, 166, 180,  23,  70, 229, 139,  29,  27,  92,\n",
              "                      104,  10, 111,  88, 121, 237, 110,  19, 124,  92,  53, 169,  77, 164,\n",
              "                      161,  23,  97, 104,  59, 236,  23, 118,  25,  84, 166, 188, 214, 122,\n",
              "                       20,  94, 131,  65, 249,  42,  62, 185,  65,  63, 152, 227, 112,  76,\n",
              "                       51, 144,  63,  26,  54, 153,  69, 226,  11,  92, 161,   7,  69,  14,\n",
              "                      189, 227,  29, 217, 214, 228,  19, 133,  55,  11,  60,  32, 230,   6,\n",
              "                      163, 170,  16, 185, 188,  89,  68, 167,  66,  66, 182, 222, 168, 254,\n",
              "                       80,  85,  21, 179,  78,  23,  69, 178, 124,  83, 101, 144,  60,   5,\n",
              "                      146,  91, 160, 162,  99, 196,  71, 193,  92, 242, 196, 231,  96, 135,\n",
              "                      209,  79, 231, 165, 117, 156, 113,  74,  66, 121,  97,  38, 114, 197,\n",
              "                      238, 147, 123,   0, 240,   5, 206,  82,  11, 102, 153, 157, 235, 141,\n",
              "                      105,  40,  28,  51, 140,  41, 213, 110, 182, 156, 220, 161, 232, 249,\n",
              "                       94, 178, 154, 249, 230, 227], dtype=torch.uint8)),\n",
              "             ('model.6.bias.codebook',\n",
              "              tensor([-4.6007e-02,  2.3518e-02, -6.0331e-04,  5.1047e-02, -2.4704e-02,\n",
              "                      -6.0405e-02,  4.3236e-02,  1.2552e-02, -1.1589e-02, -3.4247e-02,\n",
              "                       6.0846e-02,  3.4927e-02,  3.0795e-02,  6.0300e-03, -5.0698e-02,\n",
              "                       1.8209e-02, -4.0153e-02, -1.8519e-02,  3.0391e-02,  5.4713e-02,\n",
              "                      -5.1881e-03, -2.8787e-02,  4.6681e-02,  2.7067e-02,  1.6273e-02,\n",
              "                      -1.6413e-02, -2.1286e-02,  3.9370e-02, -9.6080e-03,  2.1169e-02,\n",
              "                       9.4329e-03, -3.6514e-02,  3.3986e-03, -5.2755e-02, -4.2623e-02,\n",
              "                      -3.1933e-02,  5.8415e-02, -4.7734e-02, -1.4084e-02, -5.7216e-02,\n",
              "                      -7.3722e-03,  3.2321e-02, -3.6006e-03,  3.6464e-02, -2.5594e-02,\n",
              "                       1.4905e-02,  4.8734e-02,  4.1458e-02, -5.8765e-02,  7.7953e-03,\n",
              "                       5.6956e-02,  1.9368e-03, -3.8394e-02,  1.9529e-02, -4.4407e-02,\n",
              "                      -3.0036e-02, -2.7297e-02, -1.9575e-02,  2.5747e-02,  6.1995e-02,\n",
              "                       2.8035e-02,  5.2041e-02,  4.4241e-02, -4.8827e-02,  3.8925e-02,\n",
              "                       1.1610e-02, -2.7219e-03, -6.1810e-02, -5.4108e-02, -1.4962e-02,\n",
              "                      -5.9547e-03, -2.3119e-02,  5.3856e-02, -1.2846e-03, -3.3283e-02,\n",
              "                      -5.0958e-02,  2.4910e-02,  3.7820e-02, -4.1617e-02,  5.9611e-02,\n",
              "                      -1.0452e-02,  4.0393e-02, -2.2422e-02,  1.0619e-02,  4.9791e-02,\n",
              "                       1.3982e-02, -5.7897e-02,  2.0312e-02, -3.5586e-02,  5.5728e-02,\n",
              "                      -1.3523e-02, -3.1358e-02, -3.2634e-02,  3.3439e-02, -4.6673e-02,\n",
              "                      -2.6274e-02, -3.9344e-02,  4.7873e-02,  1.7261e-02,  8.6659e-03,\n",
              "                       1.4952e-04,  3.1538e-02,  1.3278e-02, -9.2843e-03,  1.5393e-02,\n",
              "                       2.9236e-02,  4.5909e-02, -5.9754e-02,  6.6453e-03, -5.1687e-02,\n",
              "                       2.1864e-02, -4.9911e-02, -5.4772e-02,  1.9231e-02, -4.0739e-02,\n",
              "                      -8.0436e-03,  4.1942e-02,  4.7257e-02, -2.0504e-02, -1.7440e-02,\n",
              "                      -1.1125e-02, -1.2939e-02, -4.5378e-02, -5.6334e-02,  5.2417e-02,\n",
              "                       5.0740e-02,  2.4308e-03, -9.1262e-03, -3.6934e-02,  3.0955e-02,\n",
              "                       2.2726e-02, -4.3757e-02,  5.8831e-02, -3.4937e-02,  3.6986e-02,\n",
              "                       6.0206e-02,  4.1082e-02,  1.2924e-03, -3.8971e-02, -2.8327e-02,\n",
              "                      -4.6003e-03, -1.2047e-02,  1.6760e-02,  6.1377e-02, -4.7175e-02,\n",
              "                      -1.7192e-03,  3.2918e-02, -3.0939e-02, -3.7810e-02,  2.3856e-02,\n",
              "                      -1.5604e-02,  4.2780e-02, -2.2523e-03,  5.6317e-02,  3.9148e-03,\n",
              "                      -5.1257e-02, -5.2352e-02, -4.9418e-02,  1.5805e-02,  5.3482e-02,\n",
              "                      -2.0001e-02, -5.3495e-02, -3.9727e-02, -9.1184e-05, -4.0331e-03,\n",
              "                      -5.6706e-02, -4.1989e-02,  2.4571e-02,  7.3833e-03, -4.4896e-02,\n",
              "                      -6.9287e-03, -5.0332e-02,  4.3795e-02, -2.9319e-02,  1.1170e-02,\n",
              "                       3.8219e-02, -1.6053e-02, -2.1856e-02,  1.1960e-02,  1.1298e-02,\n",
              "                      -4.8092e-02, -5.8361e-02,  4.9243e-02,  5.7879e-02,  5.4247e-02,\n",
              "                       2.0723e-02,  5.1727e-02,  6.4163e-04, -1.5797e-03,  3.4653e-02,\n",
              "                      -2.9773e-02,  5.0413e-02,  3.5245e-02, -5.5422e-02,  1.9015e-02,\n",
              "                       3.5873e-02, -2.6926e-02,  2.6637e-02,  5.5318e-02,  8.1529e-03,\n",
              "                       1.4525e-02,  2.9785e-02, -5.9125e-02, -4.2303e-02,  4.2244e-02,\n",
              "                      -3.5877e-02,  4.8409e-02,  1.7677e-02,  3.0154e-03,  1.2936e-02,\n",
              "                       2.5488e-02,  6.1763e-02, -7.7170e-03, -3.1046e-04,  4.4891e-02,\n",
              "                       1.9996e-02, -2.5213e-02, -4.2949e-02, -5.5943e-02,  6.2428e-02,\n",
              "                       8.9961e-03,  1.6115e-02,  2.6098e-02,  3.9919e-02,  4.5866e-03,\n",
              "                      -1.6854e-02,  5.1300e-02, -3.3803e-02, -1.8165e-02, -3.3216e-03,\n",
              "                      -6.2407e-03, -1.0795e-02,  9.7417e-03, -1.2326e-02,  4.6342e-02,\n",
              "                       5.9118e-02, -2.7961e-02,  2.3105e-02, -5.3196e-02,  1.8610e-02,\n",
              "                       4.0734e-02, -3.0423e-02, -2.6027e-02, -2.4291e-02, -5.6392e-03,\n",
              "                       3.0362e-03, -1.7676e-04,  3.3829e-02, -6.0852e-02,  2.1393e-02,\n",
              "                      -4.7488e-02, -8.9153e-03,  4.8957e-03, -3.7422e-02,  1.6518e-02,\n",
              "                      -1.0040e-02])),\n",
              "             ('model.8.weight.indexes',\n",
              "              tensor([[223,  75,  97,  ..., 106, 252,  36],\n",
              "                      [135, 109, 149,  ...,  31, 101, 151],\n",
              "                      [106, 247,  87,  ..., 212, 107,   3],\n",
              "                      ...,\n",
              "                      [198,  31, 219,  ..., 185,  67, 241],\n",
              "                      [  6, 170,  73,  ...,  43,  75,  24],\n",
              "                      [160, 124,   0,  ...,  61, 110,  58]], dtype=torch.uint8)),\n",
              "             ('model.8.weight.codebook',\n",
              "              tensor([-2.6648e-02,  1.8126e-02, -6.2543e-03,  7.0130e-03,  2.6105e-02,\n",
              "                      -1.5743e-02,  3.2716e-03,  1.0973e-02, -1.9317e-02,  3.0184e-02,\n",
              "                      -1.0648e-02, -3.0134e-02, -1.5147e-03,  2.2242e-02,  1.3981e-02,\n",
              "                      -2.2117e-02, -8.3143e-03, -1.4334e-02,  1.1281e-03, -3.1262e-03,\n",
              "                       1.9993e-02, -2.4688e-02,  2.4158e-02, -2.7830e-02,  2.8203e-02,\n",
              "                       1.5968e-02, -1.1516e-02, -1.8002e-02,  9.4514e-03,  1.2692e-02,\n",
              "                       5.4963e-03, -2.0963e-02, -3.1492e-02,  3.1487e-02, -1.2614e-02,\n",
              "                      -2.5795e-02, -2.3795e-02, -1.8293e-04, -5.5513e-03,  1.7213e-02,\n",
              "                      -4.5554e-03, -2.8945e-02,  2.7187e-02, -7.0836e-03,  2.1346e-03,\n",
              "                      -1.6805e-02,  2.1008e-02,  1.8965e-02, -9.2270e-03,  4.1479e-03,\n",
              "                       2.2710e-02,  8.3538e-03,  1.4560e-02,  2.9259e-02,  1.1638e-02,\n",
              "                      -2.3216e-02, -2.0116e-02,  2.5281e-02, -2.6878e-03,  1.0006e-02,\n",
              "                      -1.8509e-02,  1.5433e-02, -1.3526e-02, -1.5240e-02, -2.9541e-02,\n",
              "                       2.3425e-02,  6.7530e-03, -9.6136e-04,  2.6645e-02, -9.9454e-03,\n",
              "                      -7.8529e-03,  2.6479e-02,  1.3171e-02, -1.4649e-02,  4.9748e-03,\n",
              "                      -3.6027e-03, -1.6540e-02, -2.2628e-02,  3.0940e-02,  1.6719e-02,\n",
              "                       2.4579e-02,  5.8695e-04, -2.0325e-03,  6.0174e-03, -2.5173e-02,\n",
              "                      -4.9462e-03, -2.0405e-02, -2.1644e-02,  7.8660e-03, -1.2052e-02,\n",
              "                       2.0487e-02,  2.8738e-02,  1.9491e-02,  1.9078e-03, -6.8058e-03,\n",
              "                      -2.5600e-02,  2.3673e-02, -1.7527e-02, -2.8355e-02,  2.7663e-02,\n",
              "                       2.5766e-03, -4.1126e-03,  7.7170e-05, -2.7341e-02,  2.1761e-02,\n",
              "                       8.7751e-03,  1.2169e-02, -1.9837e-02,  3.0667e-02, -2.4036e-02,\n",
              "                      -2.6215e-02,  9.7252e-03,  2.5911e-02, -3.0668e-02,  2.9739e-02,\n",
              "                      -2.4254e-02, -8.9838e-03,  2.5507e-02, -1.2869e-02, -1.0185e-02,\n",
              "                       1.0538e-02, -1.4028e-02, -7.6063e-03,  7.5797e-03, -6.5233e-03,\n",
              "                       1.8523e-02, -2.9843e-02,  2.4809e-02, -1.0873e-02,  1.6472e-02,\n",
              "                      -5.7667e-03,  1.1397e-02,  6.5054e-03,  2.1510e-02, -5.1458e-03,\n",
              "                      -1.5999e-02,  1.6646e-03,  2.6296e-02,  1.4848e-02, -1.9573e-02,\n",
              "                      -2.3516e-02,  1.5706e-02,  1.4066e-03,  5.7605e-03, -2.2362e-02,\n",
              "                      -1.8250e-02, -9.4695e-03,  2.8471e-02,  1.7696e-02,  4.4509e-03,\n",
              "                      -2.6877e-02, -2.5398e-02,  1.4275e-02, -3.0941e-02, -2.8642e-02,\n",
              "                       2.2477e-02, -1.2348e-02, -2.4935e-02, -2.7584e-02,  2.6985e-02,\n",
              "                      -2.1877e-02, -1.1772e-02, -1.2372e-03, -8.0894e-03, -2.2629e-03,\n",
              "                       2.2943e-02,  2.7956e-03, -1.6270e-02,  3.8404e-03,  5.2343e-03,\n",
              "                      -8.5368e-03,  2.3569e-03, -2.0696e-02, -1.1293e-02,  2.0747e-02,\n",
              "                       1.5144e-02, -4.4379e-04, -1.7293e-02,  1.2932e-02,  1.7459e-02,\n",
              "                      -2.8082e-02, -1.7840e-03, -1.3098e-02, -2.1421e-02, -2.9016e-03,\n",
              "                      -2.6431e-02, -2.9245e-02,  2.7932e-02, -2.7104e-02,  3.3355e-04,\n",
              "                       4.7168e-03,  1.8730e-02, -1.1086e-02, -3.1222e-02,  1.6972e-02,\n",
              "                      -2.6001e-02,  2.8996e-02, -1.4958e-02,  1.3688e-02,  1.9222e-02,\n",
              "                      -3.0403e-02,  2.5716e-02,  1.9748e-02,  7.2890e-03,  2.9507e-02,\n",
              "                      -3.3554e-03, -7.3530e-03,  2.0240e-02,  2.3924e-02, -3.8593e-03,\n",
              "                       2.4364e-02,  1.7922e-02, -1.5498e-02,  9.2016e-03, -6.9903e-04,\n",
              "                       1.0279e-02,  2.3183e-02, -1.7052e-02,  8.5698e-03, -1.0424e-02,\n",
              "                       3.5488e-03,  1.1179e-02,  8.4571e-04, -2.4462e-02,  1.8325e-02,\n",
              "                       1.2437e-02,  6.2687e-03, -9.7092e-03,  1.6226e-02, -1.8783e-02,\n",
              "                       2.9964e-02, -4.3428e-03,  3.1212e-02, -1.7759e-02, -5.3471e-03,\n",
              "                       2.6806e-02,  1.1904e-02, -4.7479e-03,  2.5049e-02,  1.0763e-02,\n",
              "                       2.7416e-02, -8.7535e-03,  3.0213e-03,  2.2005e-02, -1.3752e-02,\n",
              "                      -1.3311e-02, -2.2914e-02,  8.9818e-03, -2.4801e-03, -6.0022e-03,\n",
              "                       1.3422e-02, -2.1202e-02,  3.0414e-02, -1.9054e-02,  8.1209e-03,\n",
              "                       2.1261e-02])),\n",
              "             ('model.8.bias.indexes',\n",
              "              tensor([180, 210, 119,  57,  67, 129,  44,  50, 172,  67, 217,  72, 248,  96,\n",
              "                      216, 103, 100,  39,  69, 127, 114,  86, 176, 176,  97,  77,  60,  94,\n",
              "                      193,  69, 128, 153, 113,  52, 152, 195, 172,  18,  36,  59,   5, 249,\n",
              "                      240, 213,  10, 192, 165,  74, 141, 100, 161,  12, 134, 156,  66,  80,\n",
              "                      235, 190, 130,  65,   6,  74,  59,  68, 219,  93,  92, 185, 216, 209,\n",
              "                       61, 102, 211,  64, 206, 168,  94, 154, 222,  54, 212,  48,  32,  58,\n",
              "                      245, 178, 159, 144, 129,  22,  21,  39, 140,  26,  76, 128, 169,  49,\n",
              "                      131, 226,  95, 197, 133,  66, 202,  12, 200, 236, 167, 201, 131,  45,\n",
              "                      108,  78, 215,  44,  17, 228, 227, 166,  91,  65, 154, 254,   2, 139,\n",
              "                      212, 138,  56,   2,  49,  85,  88,  69, 114, 136, 158,  51, 109, 176,\n",
              "                      232,  52, 133,  28,  84, 203,  63,  11, 127,  22, 137,  31, 100, 234,\n",
              "                       95, 205,  23, 239,  94, 143, 252,  78, 242, 172, 235,  80,  32,  33,\n",
              "                      177, 139, 250, 134,   9, 213, 117, 178,   7, 112,  85, 169, 172,  50,\n",
              "                       46, 105, 144,  45,  68, 175,  90,  71,  23, 209,  69,  59, 119, 150,\n",
              "                      187, 242, 126, 239, 210,  45,  38,  65, 192, 213, 105, 226, 198, 117,\n",
              "                      238, 217,   7,  28, 102, 184,  42, 171,  54,  84, 174, 135, 131, 219,\n",
              "                       92,  79, 149, 251,  61, 105,  27, 198, 214, 112,  67,  80, 206, 108,\n",
              "                      253, 114, 110, 188,  63,  45, 156,  87, 196, 171,  35, 203, 251,   6,\n",
              "                      127, 206, 251,  41,   1, 233,  29,  76,  25, 177, 246,  27,  75,  71,\n",
              "                      207,  24, 101, 242,  17, 125,  96, 237, 153,  23,  91,  28,  86, 131,\n",
              "                      192, 164,  23,  98, 135, 212,  62,  95, 146, 230, 221,  30,  20,  26,\n",
              "                        7,  42,  21,  77, 224,  88, 199, 190, 249, 222, 241, 183,   6, 150,\n",
              "                        7,  91,  62,  74,  73, 197,  39,  86,  80,  59, 250,  83,  89, 154,\n",
              "                       77, 142,  42,  20,  13, 123,  56,  10,  41,  20,  55,   5,  87,   4,\n",
              "                      160, 132,  66, 213, 252, 199, 143,  39, 112,  34, 111,  47, 247, 245,\n",
              "                      230, 111, 217, 176, 180,  57,  54,  17, 106,  45, 240, 252,  96, 139,\n",
              "                       45,  24, 193, 157, 163,  93,  29, 220,  16,  21,  86, 238, 118, 248,\n",
              "                      239,  70,  30,  79,  18,  61, 204,  30, 195,  94,  13,  42,  77, 194,\n",
              "                      116,  54,  27, 235,  41,  25, 113,  93, 147, 112,  64,  84, 188,  40,\n",
              "                       58, 178,  25, 236,  10,  35, 102, 172, 190, 224, 174, 100,  60,  52,\n",
              "                       81, 202,  35,  17, 139,  84,  42, 180,  31, 133, 159, 122,  40,  99,\n",
              "                       70, 142,  11, 160,   0, 114,   4, 115, 168, 159, 148, 161, 157, 218,\n",
              "                       31, 185, 137,  96, 222, 251, 226,  28,  88, 127,  99,  81, 226, 247,\n",
              "                      118,  50, 241,  46, 162, 123, 179, 231, 144, 175, 225, 142, 219, 104,\n",
              "                      115,  43, 230, 203, 142, 211,  29, 189,  43,  16, 181,  49, 177, 101,\n",
              "                      110, 247, 248, 149,  45, 172,   4,  96, 182,   5,  27, 171,  32, 244,\n",
              "                      169,  82, 104, 196,  26,   9,  46,  35,  40, 119,  92, 159,  46, 182,\n",
              "                       78, 137, 247, 116,  28, 185,  70,  16,  79,  15, 148, 147,  92, 167,\n",
              "                      138,  41, 163,   0, 141,  24, 214, 180,  87,  49,  97,  79, 153, 154,\n",
              "                      135, 173, 100,  21, 125,  56,  99,  22,  62, 155,  48, 203, 103, 194,\n",
              "                      144,  75, 117,  45, 183,  55, 173,   7,  60,  59, 167,  70, 193, 181,\n",
              "                      180, 125, 124,  92, 102, 229, 170, 199, 201,  47, 182, 236, 204,  57,\n",
              "                      154, 133,  64, 247, 106, 203, 122,  57,  10, 190,  16,   8, 189,  92,\n",
              "                       88, 223,  94,  33,  47, 106, 232,  92,  54,   0,   2, 242,  37,  43,\n",
              "                       80,  99, 119, 221, 120, 184, 200, 222, 185,  29, 252, 155,  49, 163,\n",
              "                      119, 153, 119,  95,  89,  51, 169, 184,  46,   7, 167, 246, 100, 208,\n",
              "                       64,  24,  97,  90,  33, 166, 170,  96, 156, 195, 149,  48,  52, 196,\n",
              "                      136, 167,  96,  44,  77,  71,  30,  28, 240,  34, 100, 114, 130, 241,\n",
              "                       52,  68, 111, 199,  73,  72,  61,  29,  12,  64,  46,   6, 249, 159,\n",
              "                       34, 200, 183, 129,  71, 165,  53, 116,  91,   3, 179,  82,  82, 136,\n",
              "                       65,  28,  13, 140,  47,  51, 195,  26, 166, 110,  39, 182,  77,  25,\n",
              "                       89,  19, 221,  66, 162, 173, 213, 237, 150, 171, 255, 243,  55, 150,\n",
              "                      114, 132,  14, 181, 110, 205,  28,  90, 107, 211, 232,  58,  47,  37,\n",
              "                      140,  94, 181, 145,  78,  42, 179, 220,  34, 145, 238, 151, 146,   3,\n",
              "                      234, 202, 247, 124, 201, 121,  35,  18,  58,  52,  16, 128, 162, 120,\n",
              "                      168,  92, 186,  58, 194, 214, 239,   5,  32, 232,  72, 191,  63,   0],\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.8.bias.codebook',\n",
              "              tensor([-2.8591e-02,  9.2014e-03, -1.0426e-02,  2.4284e-02, -4.8114e-03,\n",
              "                      -1.8072e-02,  1.8007e-02,  3.6714e-03,  4.4309e-03, -2.4281e-02,\n",
              "                       1.3586e-02,  6.6337e-04,  2.4394e-02, -1.7291e-03,  2.1691e-02,\n",
              "                       2.7766e-02, -2.9943e-02,  6.2933e-03, -7.6498e-03,  2.7036e-02,\n",
              "                       1.0713e-02,  1.5573e-02, -1.3028e-02, -2.6834e-02, -1.6039e-02,\n",
              "                       1.8623e-02, -1.9931e-02, -3.1009e-02, -1.1502e-02,  2.5547e-02,\n",
              "                       1.2560e-02,  4.8858e-03, -3.5035e-03,  2.0269e-02,  1.6783e-02,\n",
              "                       2.2926e-02,  2.6406e-03,  7.5732e-03, -2.6018e-02,  1.3661e-03,\n",
              "                      -2.6105e-02, -6.4909e-03,  2.9769e-02, -2.0835e-02,  2.6733e-02,\n",
              "                       1.5702e-05, -1.7370e-02,  2.8648e-02,  3.1574e-02, -2.2259e-02,\n",
              "                      -2.7656e-02,  1.4939e-02,  6.9823e-03, -1.8905e-02,  5.6620e-03,\n",
              "                       1.1871e-02, -1.2409e-02, -2.5316e-02,  1.9889e-02, -1.3833e-02,\n",
              "                       1.0229e-02, -1.4964e-02, -2.5951e-03,  1.7281e-02, -8.1846e-03,\n",
              "                      -2.3655e-02,  8.6847e-03, -1.3393e-03,  1.1189e-02,  2.1032e-02,\n",
              "                       2.6425e-02, -1.0924e-02, -2.2693e-02, -2.9300e-02,  4.2538e-03,\n",
              "                       1.3104e-02, -5.2797e-03, -1.9312e-02,  3.1108e-02,  1.6186e-02,\n",
              "                       3.3225e-03, -1.6789e-02,  2.4850e-02, -1.0982e-02, -3.0425e-02,\n",
              "                       2.2455e-02, -1.3138e-02,  1.9196e-02, -7.1331e-03,  1.4233e-02,\n",
              "                      -1.5599e-02,  2.5962e-02, -9.5381e-03, -8.0344e-04, -1.3508e-02,\n",
              "                      -1.7696e-02,  5.2908e-03,  2.7221e-02,  1.9480e-02, -2.7271e-02,\n",
              "                       2.5194e-02, -8.7546e-03, -1.1930e-02, -2.0438e-03,  2.8927e-02,\n",
              "                      -3.0643e-03,  2.2128e-03,  2.8387e-02, -2.4659e-02, -5.6986e-03,\n",
              "                      -2.8996e-02, -2.1402e-02,  1.0603e-03, -2.3118e-02, -2.3891e-02,\n",
              "                       2.3178e-02, -1.6401e-02, -2.5562e-02,  2.1203e-02, -2.9579e-02,\n",
              "                       2.0760e-02,  9.6074e-03,  1.7506e-02, -1.8548e-02, -2.8806e-02,\n",
              "                      -1.6101e-03,  2.3811e-02,  1.5885e-02, -1.7009e-02, -3.1468e-02,\n",
              "                       2.2053e-02,  3.0519e-03, -4.5778e-03, -1.0800e-02,  3.0833e-02,\n",
              "                       1.8275e-02,  4.5884e-03, -1.4999e-03, -2.0515e-02,  8.9713e-03,\n",
              "                      -1.5343e-02,  6.6244e-03,  1.2749e-02,  1.9663e-02,  1.5093e-02,\n",
              "                      -2.2826e-02, -2.1138e-02, -9.9629e-03,  2.0522e-02, -2.5116e-04,\n",
              "                       2.7479e-02,  9.8245e-03, -2.6509e-02,  1.4651e-02,  1.0487e-02,\n",
              "                      -1.0658e-02, -2.4897e-02, -1.2494e-02, -3.8131e-03,  1.0924e-02,\n",
              "                       2.4560e-02,  1.6574e-02, -1.1167e-02,  2.7973e-02,  2.7438e-04,\n",
              "                      -1.2710e-02, -2.8416e-02, -3.1281e-02,  2.1468e-02, -2.1952e-02,\n",
              "                       1.6978e-02,  1.1370e-02, -3.0226e-02,  1.3802e-02,  1.5804e-02,\n",
              "                      -1.2200e-02,  2.7693e-03, -1.3347e-02, -1.9539e-02, -7.9794e-03,\n",
              "                       3.0538e-02, -5.1340e-03,  1.8994e-02,  2.2580e-02,  2.9437e-02,\n",
              "                      -5.4917e-03,  1.2107e-02,  1.2143e-02,  3.1369e-02,  8.6062e-03,\n",
              "                       2.1860e-02,  3.9454e-03,  1.3179e-02, -7.4026e-03,  1.5681e-03,\n",
              "                      -1.5793e-02, -1.7550e-02, -1.5078e-02,  1.5349e-02, -1.1567e-03,\n",
              "                       9.3812e-03,  2.8226e-02, -1.8687e-02, -2.4155e-02,  5.0399e-03,\n",
              "                      -2.2479e-02,  1.2917e-02, -2.4831e-03,  6.4519e-03, -1.7858e-02,\n",
              "                      -1.3636e-02,  2.9183e-03,  1.6427e-02,  6.8451e-03, -4.4364e-03,\n",
              "                       3.1209e-02, -2.2073e-03, -6.1744e-03,  1.7843e-02, -2.7025e-02,\n",
              "                       2.6172e-02,  8.0639e-03,  7.4114e-03,  2.4339e-03,  2.6953e-02,\n",
              "                       2.3971e-02,  1.7385e-02, -1.1033e-02,  9.9365e-03, -2.8746e-02,\n",
              "                      -2.7392e-02,  1.7824e-02, -1.1815e-02, -2.0665e-02, -2.0064e-02,\n",
              "                       1.8100e-03,  2.4155e-02, -1.0528e-02,  1.0106e-02, -1.4128e-02,\n",
              "                       1.7656e-02, -4.1017e-03, -1.4723e-02,  3.1265e-02,  2.3305e-02,\n",
              "                       3.6449e-04, -5.8599e-03, -1.4376e-02,  1.7127e-02,  2.9980e-02,\n",
              "                      -2.6227e-02, -2.7778e-02,  2.0614e-02, -1.8290e-02,  9.9713e-03,\n",
              "                       1.9219e-03]))])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-ou-oG-Xx1X",
        "outputId": "8ae49da3-d655-4f9b-ed79-bfc775e7c9d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoencoderBig(\n",
              "  (model): Sequential(\n",
              "    (0): HybridLinear(\n",
              "      (weight): QuantizedParams()\n",
              "      (bias): QuantizedParams()\n",
              "    )\n",
              "    (1): ReLU()\n",
              "    (2): HybridLinear(\n",
              "      (weight): QuantizedParams()\n",
              "      (bias): QuantizedParams()\n",
              "    )\n",
              "    (3): ReLU()\n",
              "    (4): HybridLinear(\n",
              "      (weight): QuantizedParams()\n",
              "      (bias): QuantizedParams()\n",
              "    )\n",
              "    (5): ReLU()\n",
              "    (6): HybridLinear(\n",
              "      (weight): QuantizedParams()\n",
              "      (bias): RegularParams()\n",
              "    )\n",
              "    (7): ReLU()\n",
              "    (8): HybridLinear(\n",
              "      (weight): QuantizedParams()\n",
              "      (bias): QuantizedParams()\n",
              "    )\n",
              "    (9): ReLU()\n",
              "    (10): HybridLinear(\n",
              "      (weight): QuantizedParams()\n",
              "      (bias): QuantizedParams()\n",
              "    )\n",
              "    (11): ReLU()\n",
              "    (12): HybridLinear(\n",
              "      (weight): QuantizedParams()\n",
              "      (bias): QuantizedParams()\n",
              "    )\n",
              "    (13): ReLU()\n",
              "    (14): HybridLinear(\n",
              "      (weight): QuantizedParams()\n",
              "      (bias): QuantizedParams()\n",
              "    )\n",
              "    (15): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_loss(model, ds_test, torch.nn.MSELoss())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-9REz9mX3pz",
        "outputId": "3e90e157-f9d5-4c5b-df60-3cb63c02ab92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01229883701344774"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first = test[0][0]\n",
        "check(first)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "ZbOiFkThYBH4",
        "outputId": "d01a7cc8-3830-4a2b-ccc1-bd1eb46a31f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"ae-big-q8.pth\")"
      ],
      "metadata": {
        "id": "wAO2ONq3YJVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%shell ls -hla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNDUWH4BZZPg",
        "outputId": "c0fbc444-0f29-42eb-c3da-10ea77baef40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 50M\n",
            "drwxr-xr-x 1 root root 4.0K Aug 22 22:37 .\n",
            "drwxr-xr-x 1 root root 4.0K Aug 22 22:25 ..\n",
            "-rw-r--r-- 1 root root 8.2M Aug 22 22:26 ae-1.pth\n",
            "-rw-r--r-- 1 root root  34M Aug 22 22:36 ae-big.pth\n",
            "-rw-r--r-- 1 root root 8.4M Aug 22 22:37 ae-big-q8.pth\n",
            "drwxr-xr-x 4 root root 4.0K Aug 21 13:33 .config\n",
            "drwxr-xr-x 3 root root 4.0K Aug 22 22:26 data\n",
            "drwxr-xr-x 1 root root 4.0K Aug 21 13:34 sample_data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MTyeEjQ4Zcm5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}